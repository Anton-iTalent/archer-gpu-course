<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<title>CUDA programming</title>

  <link rel="stylesheet" href="../dist/reset.css">
  <link rel="stylesheet" href="../dist/reveal.css">
  <link rel="stylesheet" href="../dist/theme/white.css">
  <link rel="stylesheet" href="../css/local.css">

  <!--- Theme used for syntax highlighted code --->
  <link rel="stylesheet" href="../plugin/highlight/vs.css">

</head>

<!-- Start of presentation --> 
<body>
<div class="reveal">
<div class="slides">


  <section>

    <h1>CUDA Programming</h1>
    <p>Kevin Stratford</p>
    <p> kevin@epcc.ed.ac.uk </p>
    <p>Material by: Alan Gray, Kevin Stratford </p>
    <img class = "plain" src ="../img/epcc_logo.png" alt = "EPCC Logo" />

  </section>

  <section>
    <h4> Overview </h4>

    <ul>
      <li> The model </li>
      <li> CUDA blocks and threads </li>
      <li> Kernels </li>
      <li> Data management </li>
    </ul>
  </section>

  <!--
  <section>
    <h4> Development </h4>

    <ul>
      <li> Early graphics interfaces difficult to program
      <ul>
        <li> Partcularly for scientific applications
      </ul>
      <li> CUDA developed around 2007 to ease development
      <li> C/C++ interface
      <ul>
        <li> Host side interface to control GPU memory etc
        <li> Kernels executed on the device by <i>threads</i>
      </ul>
      <li> Extended to CUDA Fortran with appropriate compiler
    </ul>
  </section>
  -->

  <section>
     <h4> Host and Device </h4>

     <ul>
       <li> Separate memory / address spaces </li>
     </ul>

      <img class = "plain" src = "./ks-schematic-host-device.svg"
          width = "60%"
           alt = "A schematic diagram showing CPU host and GPU device
                  with separate address spaces connected via (PCIe) bus">
  </section>

  <section>
    <h4> The model </h4>

    <p>
    Map the problem space onto a Cartesian grid, e.g., in one dimension
    </p>

    <img width = "80%" src = "./ks-schematic-grid.svg"
           alt = "Schemetic of a one-dimensional grid"
           class = "plain">

    <p style = "text-align: left;">
    Each element of the grid has one thread associated with it
    </p>

  </section>

  <section>
    <h4>Blocks</h4>

    <p style = "text-align: left;">
      The grid is further decomposed into blocks
    </p>

    <ul style = "display: block; float: left; width: 100%; font-size: 70%;">
      <li> All blocks have same number of threads </li>
    </ul>


    <img width = "80%" src = "./ks-schematic-block.svg"
           alt = "Schemetic of a one-dimensional set of blocks"
           class = "plain">

    <p style = "text-align: left;">
      There is a maximum number of threads per block (often 1024)
    </p>
    

  </section>

  <!--
  <section>
     <h4> Streaming Multiprocessors </h4>

     <img class = "plain" src = "./ag-schematic-sm.png" width = "60%"
          alt = "A schematic showing a GPU device made up of a number
                 of streaming multiprocessors (SMs). Each SM consists
                 of a number of cores and shared memory.">
      <dl>
         <dt> A two level hierarchy: </dt>
         <ul class = "inner">
            <li> Many streaming multiprocessors each with many cores
            <li> Exact numbers depend on particular hardware
         </ul>
       </dl>

  </section>

  <section>
     <h4> Grids, Blocks, and Threads </h4>

      <dl>
         <dt> Reflected in programming model </dt>
         <ul class = "inner">
            <li> Problem abstracted to <i>blocks</i> (map to SMs)
            <li> Each block contains a number of <i>threads</i> (map to cores)
         </ul>
         <dt> Don't care about details of mapping to hardware </dt>
         <ul class = "inner">
            <li> Just describe a grid of blocks and threads
            <li> Hardware will schedule work as it sees fit
         </ul>

       </dl>

  </section>
  -->

  <section>
     <h4><code>dim3</code> structure </h4>

     <dl style = "font-size: 100%">
     CUDA introduces a container for x,y,z dimensions
     <dt>C:</dt>
     <pre><code class = "cpp" data-trim>
     struct {
       unsigned int x;
       unsigned int y;
       unsigned int z;
     }; 
     </code></pre>
     <dt> Fortran: </dt>
     <pre><code class = "fortran" data-trim>
     type :: dim3
       integer :: x
       integer :: y
       integer :: z
     end type dim3
     </code></pre>
     </dl>
  </section>

  <section>
    <h4> Example </h4>

    <pre><code class = "cpp">
/* Consider the one-dimensional loop: */

for (int i = 0; i < LOOP_LENGTH; i++) {
   result[i] = 2*i;
}
    </code></pre>
  </section>

  <section>
    <h4> CUDA C Kernel Function </h4>

    <pre><code class = "cpp">
__global__ void myKernel(int * result) {

  int i;

  i = threadIdx.x;
  result[i] = 2*i;
}
    </code></pre>
  </section>

  <section>
    <h4> Executing a kernel </h4>

    <pre><code class = "cpp">
/* Kernel is launched by the host by specifying
 * Number of blocks (sometimes "blocksPerGrid")
 * Number of threads per block */

dim3 blocks;
dim3 threadsPerBlock;

threadsPerBlock.x = LOOP_LENGTH;
blocks.x          = 1;

myKernel &lt&lt&lt blocks, threadsPerBlock &gt&gt&gt (result);
    </code></pre>
    <dl style = "font-size: 100%">
      <dt>Referred to as the execution configuration</dt>
    </dl>
  </section>

  <section>
    <h4> CUDA Fortran  </h4>
    <p>
    <pre class = "stretch"><code class = "fortran">
! In Fortran an analogous kernel is...
 
attributes(global) subroutine myKernel(result)
  integer, dimension(:) :: result
  integer               :: i

  i = threadIdx%x
  result(i) = 2*i
end subroutine myKernel

! ... with execution ...

threadsPerBlock%x = LOOP_LENGTH
blocks%x          = 1
call myKernel &lt&lt&lt blocks, threadsPerBlock &gt&gt&gt (result)
    </code></pre>

  </section>


  <section>
    <h4> More than one block </h4>
    <p>
    <pre class = "stretch"><code class = "cpp">
/* One block only uses one SM; use of resources is very poor.
 * Usually want large arrays using many blocks. */

__global__ void myKernel(int * result) {

  int i = blockIdx.x*blockDim.x + threadIdx.x;
  result[i] = 2*i;
}

/* ... with execution ... */

threadsPerBlock.x = THREADS_PER_BLOCK;
block.x           = LOOP_LENGTH/THREADS_PER_BLOCK;

myKernel &lt&lt&lt blocks, threadsPerBlock &gt&gt&gt (result);
     </code></pre>

  </section>

  <section>
    <h4> More than one block: Fortran </h4>
    <p>
    <pre class = "stretch"><code class = "fortran">
attributes(global) subroutine myKernel(result)
  integer, dimension(:) :: result
  integer               :: i

  i = (blockIdx%x - 1)*blockDim%x + threadIdx%x
  result(i) = 2*i
end subroutine myKernel

! ... with execution ...

threadsPerBlock%x = THREADS_PER_BLOCK
blocks%x          = LOOP_LENGTH/THREADS_PER_BLOCK

call myKernel &lt&lt&lt blocks, threadsPerBlock &gt&gt&gt (result)
    </code></pre>

  </section>


  <section>
    <h4> Internal variables: C </h4>

    <dl style = "font-sze: 80%">
      <dt> All provided by the implementation:
      <ul class = "inner">
        <li> Fixed at kernel invocation:
      </ul>
    </dl>
      <pre><code class = "cpp" data-trim>
dim3 gridDim;    /* Number of blocks */
dim3 blockDim;   /* Number of threads per block */
    </code></pre>

    <dl style = "font-sze: 80%">
      <ul class = "inner">
        <li> Unique to each block:
      </ul>
    </dl>
    <pre><code class = "cpp" data-trim>
dim3 blockIdx;   /* 0 &lt= blockIdx.x &lt gridDim.x etc */
    </code></pre>

    <dl style = "font-sze: 80%">
      <ul class = "inner">
        <li> Unique to each thread:
      </ul>
    </dl>
    <pre><code class = "cpp" data-trim>
dim3 threadIdx;  /* 0 &lt= threadIdx.x &lt blockDim.x etc */
    </code></pre>

  </section>

  <section>
    <h4> Internal variables: Fortran </h4>

    <dl style = "font-sze: 80%">
      <dt> Again provided by the implementation:
      <ul class = "inner">
        <li> Fixed at kernel invocation:
      </ul>
    </dl>
      <pre><code class = "fortran" data-trim>
type (dim3) :: gridDim   ! Number of blocks
type (dim3) :: blockDim  ! Number of threads per block
    </code></pre>

    <dl style = "font-sze: 80%">
      <ul class = "inner">
        <li> Unique to each block:
      </ul>
    </dl>
    <pre><code class = "fortran" data-trim>
type (dim3) :: blockIdx  ! 1 &lt= blockIdx%x &lt= gridDim%x etc
    </code></pre>

    <dl style = "font-sze: 80%">
      <ul class = "inner">
        <li> Unique to each thread:
      </ul>
    </dl>
    <pre><code class = "fortran" data-trim>
type (dim3) :: threadIdx ! 1 &lt= threadIdx%x &lt= blockDim%x etc
    </code></pre>

  </section>



  </section>

  <section>
    <h4> Two-dimensional example </h4>
    <p>
    <pre><code class = "cpp">
__global__ void matrix2d(float a[N][N], float b[N][N],
                         float c[N][N]) {

  int j = blockIdx.x*blockDim.x + threadIdx.x;
  int i = blockIdx.y*blockDim.y + threadIdx.y;

  c[i][j] = a[i][j] + b[i][j];
}
    </code></pre>
    <pre><code class = "cpp" data-trim>
/* ... with execution, e.g.,  ... */

dim3 blocksPerGrid(N/16, N/16, 1);
dim3 threadsPerBlock(16, 16, 1);

matrix2d &lt&lt&lt blocksPerGrid, threadsPerBlock &gt&gt&gt (a, b, c);
    </code></pre>

  </section>

  <section>
    <h4> Synchronisation between host and device </h4>

    <dl style = "font-size: 80%">
      <dt> Kernel launches are asynchronous </dt>
      <ul class = "inner">
        <li> Return immediately on the host
        <li> Synchronisation required to ensure completion
        <li> Errors can appear asynchronously!
        <!-- Mananged memory -->
      </ul>
    </dl>
    <pre><code class = "cpp">
myKernel &lt&lt&ltblocksPerGrid, threadsPerBlock&gt&gt&gt (...)

/* ... could perform independent work here ... */

err = cudaDeviceSynchronize();

/* ... now safe to obtain results of kernel ... */
    </code></pre>

    <dl style = "font-size: 80%">
      <dt> Many other CUDA operations have asynchronous analogues </dt>
      <ul class = "inner">
        <li> <code>cudaMemcpyAsync()</code>, ...
      </ul>
    </dl>

  </section>

  <section>

    <h4> Kernels: general </h4>

    <p style = "text-align: left;">
      Choose a suitable dimension for grid
    </p>
      <ul style = "display: block; float: left; width: 100%; font-size: 70%;">
	<li> One, two or three dimensions </li>
      </ul>

    <p style = "text-align: left; padding-top: 70px;">
      Fix a number of threads per block
    </p>
      <ul style = "display: block; float: left; width: 100%; font-size: 70%;">
	<li> Often 128, 256, 512, or 1024 </li>
      </ul>

    <p style = "text-align: left; padding-top: 70px;">
      Work out the number of blocks needed
    </p>
      <ul style = "display: block; float: left; width: 100%; font-size: 70%;">
	<li> Must cover the problem space </li>
	<li> May need to handle additional threads in kernel </li>
      </ul>

  </section>
  <!---
  <section>
    <h4> Synchronisation on the device </h4>

    <dl style = "font-size: 70%">
      <dt> Synchronisation between threads in the same block is possible
      <ul class = "inner">
        <li> Allows co-ordination of action in shared memory </li>
        <li> Allows reductions </li>
      </ul>
      <dt> Historically, not possible to synchronise between blocks
      <ul class = "inner">
        <li> Can only exit the kernel
        <li> Synchronise on host and start another kernel
      </ul>
    </dl>

  </section>
  -->

  <section>
    <h4> Memory Management </h4>

    <dl style = "font-size: 80%">
      <dt> Recall host and device have separate address spaces
      <ul class = "inner">
        <li> Data accessed by kernel must be in the device memory
        <li> This is managed largely explicitly
        <!-- Mananged memory -->
      </ul>
    </dl>

  </section>


  <section>
    <h4> Memory Allocation: C </h4>

    <dl style = "font-size: 80%">
      <dt> Allocation managed via standard C pointers
    </dl>
    <pre><code class = "cpp">
/* For example, provide an allocation of "nSize" floats
 * in the device memory: */

float * data;

err = cudaMalloc(&data, nSize*sizeof(float));

...

err = cudaFree(data);
    </code></pre>
    <dl style = "font-size: 80%">
      <dt> Such pointers cannot be dereferenced on the host
    </dl>

  </section>

  <section>
    <h4> Memory Movement: <code>cudaMemcpy()</code> </h4>

    <dl style = "font-size: 80%">
      <dt> Initiated on the host:
    </dl>
    <pre><code class = "cpp" data-trim>

/* Copy host data values to device memory ... */
err = cudaMemcpy(dataDevice, dataHost, nSize*sizeof(float),
                 cudaMemcpyHostToDevice);

/* And back again ... */
err = cudaMemcpy(dataHost, dataDevice, nSize*sizeof(float),
                 cudaMemcpyDeviceToHost);
    </code></pre>

    <dl style = "font-size: 80%">
      <dt> API:
    </dl>
    <pre><code class = "cpp" data-trim>
cudaError_t cudaMemcpy(void * dest, const void * src,
                       size_t count,
                       cudaMemcpyKind kind);
     </code></pre>
  </section>

  <section>

    <h4> Memory allocation: CUDA Fortran </h4>

    <dl style = "font-size: 80%">
      <dt> Declare variable to be in the device memory space
      <ul class = "inner">
        <li> Via the <code> device</code> attribute
        <li> Compiler then knows that the variable should be treated
             appropriately
      </ul>
    </dl>
    <pre><code class = "fortran">
! Make an allocation in device memory:
real, device, allocatable :: dataDevice(:)

allocate(dataDevice(nSize), stat = ...)

...

deallocate(dataDevice)
    </code></pre>


    <dl style = "font-size: 80%">
      <dt> Or, can use the C-like API </dt>
      <ul class = "inner">
        <li> <code>cudaMalloc()</code>, <code>cudaFree()</code>
      </ul>
    </dl>
    
  </section>

  <section>
    <h4> Memory movement: CUDA Fortran </h4>

    <dl style = "font-size: 80%">
      <dt> May be performed via simple assignment
      <ul class = "inner">
        <li> Again, compiler knows what action to take via declarations
      </ul>
    </dl>
    <pre><code class = "fortran">
! Copy from host to device

dataDevice(:) = dataHost(:)

! ... and back again ...

dataHost(:) = dataDevice(:)
    </code></pre>

    <dl style = "font-size: 80%">
      <dt> Can be more explicit using C-like API
    </dl>
    <pre><code class = "fortran" data-trim>
err = cudaMemcpy(dataDevice, dataHost, nSize,
                 cudaMemcpyHostToDevice)
    </code></pre>

  </section>

  <section>
    <h4> Compilation </h4>

    <dl style = "font-size: 80%">
      <dt> CUDA C source </dt>
      <ul class = "inner">
         <li> File extension <code>.cu</code> by convention </li>
         <li> Compiled via NVIDIA <code>nvcc</code></li>
      </ul>
    </dl>
    <pre><code class = "bash" data-trim>
$ nvcc -o example example.cu
    </code></pre>

    <dl style = "font-size: 80%">
      <dt> CUDA Fortran source </dt>
      <ul class = "inner">
         <li> File extension <code>.cuf</code> by convention
         <li> Compiled via Portland Group compiler <code>pgf90</code>
         <li> Use <code>-Mcuda</code> (if file extension not <code>.cuf</code>)
      </ul>
    </dl>

    <pre><code class = "bash" data-trim>
$ pgf90 -Mcuda -o example example.cuf
    </code></pre>

  </section>

  <section>

    <h4><code>pyCUDA</code></h4>

    <pre><code class = "python" data-trim>
	from pycuda.compiler import SourceModule

	kernel_code = SourceModule("""
	  __global__ void my_kernel(...) {
	    /* C code */
	  }
	""")
    </code></pre>
    <pre><code class = "python" data-trim>

	# Compile
	kernel = kernel_code.get_function("my_kernel")

	# Execute kernel
	# block and grid are tuples describing the execution

	kernel(..., block = threads_per_block, grid = blocks)
    </code></pre>

  </section>

  <section>
    <h4><code> pyCUDA </code></h4>
    <pre><code class = "python" data-trim>
	import numpy
	import pycuda.driver as cuda
	import pycuda.autoinit

	# Establish data as numpy objects
	x = numpy.zeros(ARRAY_SIZE, dtype = numpy.double)
    </code></pre>
    <pre><code class = "python" data-trim>
    
	# Allocate device copy
	# Copy to device, and copy back again

	x_d = cuda.mem_alloc(x.bytes)

	cuda.memcpy_htod(x_d, x)

	kernel(x_d, ...)

	cuda.memcpy_dtoh(x, x_d)
    </code></pre>

  </section>

  <section>
    <h4> Summary </h4>

    <dl style = "font-size: 80%">
      <dt> CUDA C and CUDA Fortran </dt>
      <ul class = "inner">
         <li> Provide API and extensions for programming NVIDIA GPUs
         <li> Memory management
         <li> Kernel execution
      </ul>
      <dt> CUDA emcompasses wide range of functionality </dt>
      <ul class = "inner">
         <li> Can make significant progress with a small subset
      </ul>
      <dt> Still evolving (along with hardware) </dt>
      <ul class = "inner">
         <li> Currently CUDA v10
         <li> About one release per year
         <li> Features can be deprecated / become outmoded
      </ul>

    </dl>

  </section>

</div>
</div>

<!-- End of presentation -->

  <script src="../dist/reveal.js"></script>
  <script src="../plugin/notes/notes.js"></script>
  <script src="../plugin/markdown/markdown.js"></script>
  <script src="../plugin/highlight/highlight.js"></script>
  <script src="../plugin/math/math.js"></script>

  <script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,
        controls: false,
        center: false,
        slideNumber: 'c/t',
        mathjax2: {
            mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js',
            config: 'TeX-AMS_HTML-full'},
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes,
                   RevealMath.MathJax2],
    });
  </script>

</body>
</html>
