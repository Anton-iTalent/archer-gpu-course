{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a CUDA application\n",
    "\n",
    "In this self-paced, hands-on lab, we will take an existing CUDA application and go through several optimization steps, measuring the performance benefits of each. We will see the importance of minimizing data transfer, enabling coalesced memory access, and tuning the parallel decomposition. \n",
    "\n",
    "Lab content created by EPCC, The University of Edinburgh. Documentation and source code copyright The University of Edinburgh 2016. Lab style and template created by NVIDIA, see https://nvidia.qwiklab.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "First, please try and execute the below command. Give focus to the cell by clicking on it, and then either press the play button above or press your Enter key whilst holding down Shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"This command is running on host $HOSTNAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This exercise involves performing a series of optimizations to an existing CUDA application. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "You start with an image that looks like this:\n",
    "\n",
    "<img src=images/input.jpeg width=400>\n",
    "</td>\n",
    "<td>\n",
    "Which has been generated from the original:\n",
    "\n",
    "<img src=images/EDINB00034_2048x2048.jpg width=400>\n",
    "\n",
    "\n",
    "</td>\n",
    "\n",
    "<tr>\n",
    "</table>\n",
    "On the left is an image of Edinburgh Castle, processed such that the edges between light and dark areas replace the original picture. Your job is to reconstruct the initial image. This is an artificial thing to do, but it mimics many scientific applications (e.g. that solve systems of PDEs) since the algorithm is iterative, requiring many successive <i>stencil</i> operations. Each pixel of the new <it>image</it> is generated based on it's neighboring pixel values and the original <it>edge</it> data by repeatedly performing the following update:  \n",
    "\n",
    "image<sub>i,j</sub> = (image<sub>i-1,j</sub> + image<sub>i+1,j</sub> + image<sub>i,j-1</sub> + image<sub>i,j+1</sub> - edge<sub>i,j</sub>)/4 \n",
    "\n",
    "The more iterations, the better the reconstruction (although for simplicity we work in greyscale rather\n",
    "than colour).\n",
    "\n",
    "You are provided with a working but slow CUDA implementation of the reconstruction algorithm. First of all, letâ€™s compile and run the code. The code is set up to run the algorithm on both the GPU and the CPU. It compares the outputs from the two runs to verify correctness, and then displays timings for each run. Choose to work with either C or Fortran:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "<b>C:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a link to the C version of the templates\n",
    "!rm -rf src; ln -s src_c src; echo \"Using C version\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "<b>Fortran:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a link to the Fortran version of the templates\n",
    "!rm -rf src; ln -s src_fortran src; echo \"Using Fortran version\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Now, build and run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to compile the reconstruction code. Wait until \"Complete\" is printed in the output \n",
    "!cd src; make clean; make; cd ..; echo \"Complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to run the code. Wait until \"Complete\" is printed in the output \n",
    "!cd src; ./reconstruct; cd ..; echo \"Complete\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "View the resulting image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to view the resulting image\n",
    "!cd src; pgmtoppm white output.pgm > output.ppm; ppmtojpeg output.ppm > output.jpeg; cd ..\n",
    "from IPython.core.display import Image\n",
    "Image('src/output.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you can see that the picture is starting to become clearer. As the algorithm is iterative, there is a loop in the main routine that invokes the kernel N times where N is set to 100. Increasing N will increase the quality of the reconstruction, but please don't do this during the lab to avoid hogging the resources. If you were to run for 10 million iterations, the resulting image would look like this:\n",
    "\n",
    "<img src=images/output10M.jpeg width=400 align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to optimise the code and improve on the GPU timing printed when you ran the code, by editing the source code. Below is a window where you can browse and edit the code. Note that a one pixel wide <it>halo</it> region of zeroes is added to each edge of the various image-data arrays; this simplifies the computation as it allows the edge pixels to be treated in the same manner as other pixels. (The edge array, which holds the original edge data, does not have require a halo.)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Edit the source\n",
    "Using the notebook (you will need a separate tab or window), navigate to the appropriate source file directory for this exercise (<code>src_c</code> or <code>src_fortran</code>) and make appropriate changes to the code. The following sections contain detailed instructions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Data Transfer \n",
    "\n",
    "A challenge with GPUs and other accelerators is that transferring data between host memory and device memory is often relatively slow.  An important optimization technique involves minimise the amount of data that is transferred between host and device.\n",
    "\n",
    "Notice that in the main loop in <code>reconstruct.cu</code> (C) or <code>reconstruct.cuf</code> (Fortran), the data is copied from GPU memory to host memory and then back to GPU memory at each iteration. This is not in fact necessary; with the exception of the final iteration when the data must be copied back to the host, it is going to be processed on the GPU again in the next iteration. Therefore, we can optimise manipulating the GPU memory without directly without expensive transfers.\n",
    "\n",
    "We can simply copy the output array directly to the input array after each iteration.\n",
    "In order to do this you will need to:\n",
    "\n",
    "-----\n",
    "<b>C:</b>\n",
    "\n",
    "<ul>\n",
    "<li> remove the <code>cudaMemcpy</code> calls from inside the main loop.\n",
    "<li> replace them with a <code>cudaMemcpy</code> call to copy, directly on the device, from <code>d_output</code> to\n",
    "  <code>d_input</code>. \n",
    "<li> add a new <code>cudaMemcpy</code> call after the end of the loop (in\n",
    "  between the two calls to <code>get_current_time()</code>) to copy the\n",
    "  final result back from the GPU to the <code>output</code> buffer in\n",
    "  host memory. \n",
    "</ul>\n",
    "\n",
    "-----\n",
    "<b>Fortran:</b>\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li> remove the assignments to <code>output</code> (from <code>d_output</code>) and to <code>d_input</code> (from <code>output</code>), inside the main loop.\n",
    "<li> replace them with an assignment directly from <code>d_output</code> to <code>d_input</code>.\n",
    "<li> add a new assignment after the end of the loop (in between the two calls to <code>cpu_time()</code>) to copy\n",
    "the final result back from the GPU to the <code>output</code> buffer in host memory.\n",
    "</ul>\n",
    "\n",
    "-----\n",
    "Once you have made these changes, compile and run the code again by re-executing the corresponding cells above, and take note of the time taken by the GPU version. How does it compare to the previous timing? (The server may have different types of GPUs installed, so it is possible that the GPU in use may change from run to run. For accurate timings, make sure that the same type of GPU is being used across different runs by checking the text printed at the start of the output. See the end of the \"Intro\" lab to see how to restrict your run to a certain GPU.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Coalesced Memory Accesses\n",
    "\n",
    "The GPU performs best when consecutive CUDA threads access consecutive memory locations, allowing memory <it>coalescing</it>.\n",
    "\n",
    "-----\n",
    "<b>C:</b>\n",
    "\n",
    "However, for the kernel in <code>reconstruct_kernels.cu</code>, it can be seen that consecutive threads correspond to consecutive rows of the image, but consecutive memory locations instead correspond to \n",
    "consecutive columns. The threads are not reading from consecutive locations.\n",
    "\n",
    "-------\n",
    "\n",
    "<b>Fortran:</b>\n",
    "\n",
    "However, for the kernel in <code>reconstruct_kernels.cuf</code>, it can be seen that consecutive threads correspond to consecutive columns of the image, but consecutive memory locations instead correspond to \n",
    "consecutive rows. The threads are not reading from consecutive locations.\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "* Update the kernel such that the role of the image row and column is reversed, in relation to how pixels are assigned to CUDA threads.\n",
    "* Since the image is perfectly square, you will not need to change the way the kernel is launched. \n",
    "* How does the performance compare to the previous version?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Occupancy\n",
    "\n",
    "You should hopefully have seen a noticeable improvement in performance as a result of the changes you made to reduce data transfers between the host and the device and to enable coalescing. However, the current solution is still sub-optimal as it will not create sufficient threads to utilise all the SMs on the GPU - it has low <it>occupancy</it>.\n",
    "\n",
    "GPU codes typically run best when there are many threads running in parallel, each doing a small part of the work. We can achieve this with our image processing code by using a thread for each pixel of the image, rather than for each row or column as before. CUDA supports 1-, 2- or 3-dimensional decompositions. A 2D decomposition maps most naturally onto the pixels of an image.\n",
    "\n",
    "* Update your both your kernel, and the code responsible for specifying the decomposition such that that a 2D decomposition is over both rows and columns. \n",
    "* The original code uses 256 threads per block in a 1D CUDA decomposition. Replace this with 16 threads in each of the X and Y directions of the 2D CUDA decomposition, to give a total of 256 threads per block. Ensure that the number of blocks is specified appropriately in each direction. \n",
    "* Ensure that you retain memory coalescing\n",
    "* Again, measure performance and compare to the previous versions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Grid and Block Sizes\n",
    "\n",
    "Once you have the 2D kernel working correctly, you can try altering certain parameters and see what effect this has on its performance. In particular, you can investigate the effects of different grid and block\n",
    "sizes. How does changing the grid and block sizes affect the total runtime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"post-lab\"></a>\n",
    "## Keeping a copy of your work\n",
    "\n",
    "If you wish to keep a copy of the files related to this exercise:\n",
    "\n",
    "0. Use the notebook File menu at the top right to download the files you want, OR\n",
    "1. Save this IPython Notebook by going to `File -> Download as -> IPython (.ipynb)` (or instead choose an html copy) at the top of this window.\n",
    "2. You can execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f reconstruct.zip; zip -r reconstruct.zip src; echo \"Complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above cell, you should be able to download the zip file [here](/files/usertemplate/reconstruct/reconstruct.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "p.hint_trigger{\n",
    "  margin-bottom:7px;\n",
    "  margin-top:-5px;\n",
    "  background:#64E84D;\n",
    "}\n",
    ".toggle_container{\n",
    "  margin-bottom:0px;\n",
    "}\n",
    ".toggle_container p{\n",
    "  margin:2px;\n",
    "}\n",
    ".toggle_container{\n",
    "  background:#f0f0f0;\n",
    "  clear: both;\n",
    "  font-size:100%;\n",
    "}\n",
    "</style>\n",
    "<script>\n",
    "$(\"p.hint_trigger\").click(function(){\n",
    "   $(this).toggleClass(\"active\").next().slideToggle(\"normal\");\n",
    "});\n",
    "   \n",
    "$(\".toggle_container\").hide();\n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
